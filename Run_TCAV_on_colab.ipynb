{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "tcavenv",
      "language": "python",
      "name": "tcavenv"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "colab": {
      "name": "Run TCAV on colab.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mugheesahmad/tcav/blob/master/Run_TCAV_on_colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h3n6NUDcKWUC",
        "colab_type": "code",
        "outputId": "150c2af6-da07-4d81-b19c-866000606957",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "%tensorflow_version 1.x\n",
        "#The default TensorFlow version in Colab switched from 1.x to 2.x on the 27th of March, 2020."
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "`%tensorflow_version` only switches the major version: 1.x or 2.x.\n",
            "You set: `1.x #https://colab.research.google.com/notebooks/tensorflow_version.ipynb`. This will be interpreted as: `1.x`.\n",
            "\n",
            "\n",
            "TensorFlow 1.x selected.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LR4MON61J8t5",
        "colab_type": "code",
        "outputId": "cf066415-5ca7-4951-fbe0-1ca749e63556",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "source": [
        "# Clone the entire repo.\n",
        "!git clone https://github.com/tensorflow/tcav.git tcav\n",
        "%cd tcav\n",
        "!ls"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'tcav'...\n",
            "remote: Enumerating objects: 88, done.\u001b[K\n",
            "remote: Counting objects:   1% (1/88)\u001b[K\rremote: Counting objects:   2% (2/88)\u001b[K\rremote: Counting objects:   3% (3/88)\u001b[K\rremote: Counting objects:   4% (4/88)\u001b[K\rremote: Counting objects:   5% (5/88)\u001b[K\rremote: Counting objects:   6% (6/88)\u001b[K\rremote: Counting objects:   7% (7/88)\u001b[K\rremote: Counting objects:   9% (8/88)\u001b[K\rremote: Counting objects:  10% (9/88)\u001b[K\rremote: Counting objects:  11% (10/88)\u001b[K\rremote: Counting objects:  12% (11/88)\u001b[K\rremote: Counting objects:  13% (12/88)\u001b[K\rremote: Counting objects:  14% (13/88)\u001b[K\rremote: Counting objects:  15% (14/88)\u001b[K\rremote: Counting objects:  17% (15/88)\u001b[K\rremote: Counting objects:  18% (16/88)\u001b[K\rremote: Counting objects:  19% (17/88)\u001b[K\rremote: Counting objects:  20% (18/88)\u001b[K\rremote: Counting objects:  21% (19/88)\u001b[K\rremote: Counting objects:  22% (20/88)\u001b[K\rremote: Counting objects:  23% (21/88)\u001b[K\rremote: Counting objects:  25% (22/88)\u001b[K\rremote: Counting objects:  26% (23/88)\u001b[K\rremote: Counting objects:  27% (24/88)\u001b[K\rremote: Counting objects:  28% (25/88)\u001b[K\rremote: Counting objects:  29% (26/88)\u001b[K\rremote: Counting objects:  30% (27/88)\u001b[K\rremote: Counting objects:  31% (28/88)\u001b[K\rremote: Counting objects:  32% (29/88)\u001b[K\rremote: Counting objects:  34% (30/88)\u001b[K\rremote: Counting objects:  35% (31/88)\u001b[K\rremote: Counting objects:  36% (32/88)\u001b[K\rremote: Counting objects:  37% (33/88)\u001b[K\rremote: Counting objects:  38% (34/88)\u001b[K\rremote: Counting objects:  39% (35/88)\u001b[K\rremote: Counting objects:  40% (36/88)\u001b[K\rremote: Counting objects:  42% (37/88)\u001b[K\rremote: Counting objects:  43% (38/88)\u001b[K\rremote: Counting objects:  44% (39/88)\u001b[K\rremote: Counting objects:  45% (40/88)\u001b[K\rremote: Counting objects:  46% (41/88)\u001b[K\rremote: Counting objects:  47% (42/88)\u001b[K\rremote: Counting objects:  48% (43/88)\u001b[K\rremote: Counting objects:  50% (44/88)\u001b[K\rremote: Counting objects:  51% (45/88)\u001b[K\rremote: Counting objects:  52% (46/88)\u001b[K\rremote: Counting objects:  53% (47/88)\u001b[K\rremote: Counting objects:  54% (48/88)\u001b[K\rremote: Counting objects:  55% (49/88)\u001b[K\rremote: Counting objects:  56% (50/88)\u001b[K\rremote: Counting objects:  57% (51/88)\u001b[K\rremote: Counting objects:  59% (52/88)\u001b[K\rremote: Counting objects:  60% (53/88)\u001b[K\rremote: Counting objects:  61% (54/88)\u001b[K\rremote: Counting objects:  62% (55/88)\u001b[K\rremote: Counting objects:  63% (56/88)\u001b[K\rremote: Counting objects:  64% (57/88)\u001b[K\rremote: Counting objects:  65% (58/88)\u001b[K\rremote: Counting objects:  67% (59/88)\u001b[K\rremote: Counting objects:  68% (60/88)\u001b[K\rremote: Counting objects:  69% (61/88)\u001b[K\rremote: Counting objects:  70% (62/88)\u001b[K\rremote: Counting objects:  71% (63/88)\u001b[K\rremote: Counting objects:  72% (64/88)\u001b[K\rremote: Counting objects:  73% (65/88)\u001b[K\rremote: Counting objects:  75% (66/88)\u001b[K\rremote: Counting objects:  76% (67/88)\u001b[K\rremote: Counting objects:  77% (68/88)\u001b[K\rremote: Counting objects:  78% (69/88)\u001b[K\rremote: Counting objects:  79% (70/88)\u001b[K\rremote: Counting objects:  80% (71/88)\u001b[K\rremote: Counting objects:  81% (72/88)\u001b[K\rremote: Counting objects:  82% (73/88)\u001b[K\rremote: Counting objects:  84% (74/88)\u001b[K\rremote: Counting objects:  85% (75/88)\u001b[K\rremote: Counting objects:  86% (76/88)\u001b[K\rremote: Counting objects:  87% (77/88)\u001b[K\rremote: Counting objects:  88% (78/88)\u001b[K\rremote: Counting objects:  89% (79/88)\u001b[K\rremote: Counting objects:  90% (80/88)\u001b[K\rremote: Counting objects:  92% (81/88)\u001b[K\rremote: Counting objects:  93% (82/88)\u001b[K\rremote: Counting objects:  94% (83/88)\u001b[K\rremote: Counting objects:  95% (84/88)\u001b[K\rremote: Counting objects:  96% (85/88)\u001b[K\rremote: Counting objects:  97% (86/88)\u001b[K\rremote: Counting objects:  98% (87/88)\u001b[K\rremote: Counting objects: 100% (88/88)\u001b[K\rremote: Counting objects: 100% (88/88), done.\u001b[K\n",
            "remote: Compressing objects:   1% (1/64)\u001b[K\rremote: Compressing objects:   3% (2/64)\u001b[K\rremote: Compressing objects:   4% (3/64)\u001b[K\rremote: Compressing objects:   6% (4/64)\u001b[K\rremote: Compressing objects:   7% (5/64)\u001b[K\rremote: Compressing objects:   9% (6/64)\u001b[K\rremote: Compressing objects:  10% (7/64)\u001b[K\rremote: Compressing objects:  12% (8/64)\u001b[K\rremote: Compressing objects:  14% (9/64)\u001b[K\rremote: Compressing objects:  15% (10/64)\u001b[K\rremote: Compressing objects:  17% (11/64)\u001b[K\rremote: Compressing objects:  18% (12/64)\u001b[K\rremote: Compressing objects:  20% (13/64)\u001b[K\rremote: Compressing objects:  21% (14/64)\u001b[K\rremote: Compressing objects:  23% (15/64)\u001b[K\rremote: Compressing objects:  25% (16/64)\u001b[K\rremote: Compressing objects:  26% (17/64)\u001b[K\rremote: Compressing objects:  28% (18/64)\u001b[K\rremote: Compressing objects:  29% (19/64)\u001b[K\rremote: Compressing objects:  31% (20/64)\u001b[K\rremote: Compressing objects:  32% (21/64)\u001b[K\rremote: Compressing objects:  34% (22/64)\u001b[K\rremote: Compressing objects:  35% (23/64)\u001b[K\rremote: Compressing objects:  37% (24/64)\u001b[K\rremote: Compressing objects:  39% (25/64)\u001b[K\rremote: Compressing objects:  40% (26/64)\u001b[K\rremote: Compressing objects:  42% (27/64)\u001b[K\rremote: Compressing objects:  43% (28/64)\u001b[K\rremote: Compressing objects:  45% (29/64)\u001b[K\rremote: Compressing objects:  46% (30/64)\u001b[K\rremote: Compressing objects:  48% (31/64)\u001b[K\rremote: Compressing objects:  50% (32/64)\u001b[K\rremote: Compressing objects:  51% (33/64)\u001b[K\rremote: Compressing objects:  53% (34/64)\u001b[K\rremote: Compressing objects:  54% (35/64)\u001b[K\rremote: Compressing objects:  56% (36/64)\u001b[K\rremote: Compressing objects:  57% (37/64)\u001b[K\rremote: Compressing objects:  59% (38/64)\u001b[K\rremote: Compressing objects:  60% (39/64)\u001b[K\rremote: Compressing objects:  62% (40/64)\u001b[K\rremote: Compressing objects:  64% (41/64)\u001b[K\rremote: Compressing objects:  65% (42/64)\u001b[K\rremote: Compressing objects:  67% (43/64)\u001b[K\rremote: Compressing objects:  68% (44/64)\u001b[K\rremote: Compressing objects:  70% (45/64)\u001b[K\rremote: Compressing objects:  71% (46/64)\u001b[K\rremote: Compressing objects:  73% (47/64)\u001b[K\rremote: Compressing objects:  75% (48/64)\u001b[K\rremote: Compressing objects:  76% (49/64)\u001b[K\rremote: Compressing objects:  78% (50/64)\u001b[K\rremote: Compressing objects:  79% (51/64)\u001b[K\rremote: Compressing objects:  81% (52/64)\u001b[K\rremote: Compressing objects:  82% (53/64)\u001b[K\rremote: Compressing objects:  84% (54/64)\u001b[K\rremote: Compressing objects:  85% (55/64)\u001b[K\rremote: Compressing objects:  87% (56/64)\u001b[K\rremote: Compressing objects:  89% (57/64)\u001b[K\rremote: Compressing objects:  90% (58/64)\u001b[K\rremote: Compressing objects:  92% (59/64)\u001b[K\rremote: Compressing objects:  93% (60/64)\u001b[K\rremote: Compressing objects:  95% (61/64)\u001b[K\rremote: Compressing objects:  96% (62/64)\u001b[K\rremote: Compressing objects:  98% (63/64)\u001b[K\rremote: Compressing objects: 100% (64/64)\u001b[K\rremote: Compressing objects: 100% (64/64), done.\u001b[K\n",
            "Receiving objects:   0% (1/588)   \rReceiving objects:   1% (6/588)   \rReceiving objects:   2% (12/588)   \rReceiving objects:   3% (18/588)   \rReceiving objects:   4% (24/588)   \rReceiving objects:   5% (30/588)   \rReceiving objects:   6% (36/588)   \rReceiving objects:   7% (42/588)   \rReceiving objects:   8% (48/588)   \rReceiving objects:   9% (53/588)   \rReceiving objects:  10% (59/588)   \rReceiving objects:  11% (65/588)   \rReceiving objects:  12% (71/588)   \rReceiving objects:  13% (77/588)   \rReceiving objects:  14% (83/588)   \rReceiving objects:  15% (89/588)   \rReceiving objects:  16% (95/588)   \rReceiving objects:  17% (100/588)   \rReceiving objects:  18% (106/588)   \rReceiving objects:  19% (112/588)   \rReceiving objects:  20% (118/588)   \rReceiving objects:  21% (124/588)   \rReceiving objects:  22% (130/588)   \rReceiving objects:  23% (136/588)   \rReceiving objects:  24% (142/588)   \rReceiving objects:  25% (147/588)   \rReceiving objects:  26% (153/588)   \rReceiving objects:  27% (159/588)   \rReceiving objects:  28% (165/588)   \rReceiving objects:  29% (171/588)   \rReceiving objects:  30% (177/588)   \rReceiving objects:  31% (183/588)   \rReceiving objects:  32% (189/588)   \rReceiving objects:  33% (195/588)   \rReceiving objects:  34% (200/588)   \rReceiving objects:  35% (206/588)   \rReceiving objects:  36% (212/588)   \rReceiving objects:  37% (218/588)   \rReceiving objects:  38% (224/588)   \rReceiving objects:  39% (230/588)   \rReceiving objects:  40% (236/588)   \rReceiving objects:  41% (242/588)   \rReceiving objects:  42% (247/588)   \rReceiving objects:  43% (253/588)   \rReceiving objects:  44% (259/588)   \rReceiving objects:  45% (265/588)   \rReceiving objects:  46% (271/588)   \rReceiving objects:  47% (277/588)   \rReceiving objects:  48% (283/588)   \rReceiving objects:  49% (289/588)   \rReceiving objects:  50% (294/588)   \rReceiving objects:  51% (300/588)   \rReceiving objects:  52% (306/588)   \rReceiving objects:  53% (312/588)   \rReceiving objects:  54% (318/588)   \rReceiving objects:  55% (324/588)   \rReceiving objects:  56% (330/588)   \rReceiving objects:  57% (336/588)   \rReceiving objects:  58% (342/588)   \rReceiving objects:  59% (347/588)   \rReceiving objects:  60% (353/588)   \rReceiving objects:  61% (359/588)   \rReceiving objects:  62% (365/588)   \rReceiving objects:  63% (371/588)   \rReceiving objects:  64% (377/588)   \rReceiving objects:  65% (383/588)   \rReceiving objects:  66% (389/588)   \rReceiving objects:  67% (394/588)   \rReceiving objects:  68% (400/588)   \rReceiving objects:  69% (406/588)   \rReceiving objects:  70% (412/588)   \rReceiving objects:  71% (418/588)   \rReceiving objects:  72% (424/588)   \rReceiving objects:  73% (430/588)   \rReceiving objects:  74% (436/588)   \rReceiving objects:  75% (441/588)   \rReceiving objects:  76% (447/588)   \rReceiving objects:  77% (453/588)   \rReceiving objects:  78% (459/588)   \rReceiving objects:  79% (465/588)   \rremote: Total 588 (delta 44), reused 54 (delta 20), pack-reused 500\u001b[K\n",
            "Receiving objects:  80% (471/588)   \rReceiving objects:  81% (477/588)   \rReceiving objects:  82% (483/588)   \rReceiving objects:  83% (489/588)   \rReceiving objects:  84% (494/588)   \rReceiving objects:  85% (500/588)   \rReceiving objects:  86% (506/588)   \rReceiving objects:  87% (512/588)   \rReceiving objects:  88% (518/588)   \rReceiving objects:  89% (524/588)   \rReceiving objects:  90% (530/588)   \rReceiving objects:  91% (536/588)   \rReceiving objects:  92% (541/588)   \rReceiving objects:  93% (547/588)   \rReceiving objects:  94% (553/588)   \rReceiving objects:  95% (559/588)   \rReceiving objects:  96% (565/588)   \rReceiving objects:  97% (571/588)   \rReceiving objects:  98% (577/588)   \rReceiving objects:  99% (583/588)   \rReceiving objects: 100% (588/588)   \rReceiving objects: 100% (588/588), 493.45 KiB | 2.48 MiB/s, done.\n",
            "Resolving deltas:   0% (0/356)   \rResolving deltas:   1% (4/356)   \rResolving deltas:   4% (17/356)   \rResolving deltas:   5% (20/356)   \rResolving deltas:   6% (22/356)   \rResolving deltas:   7% (25/356)   \rResolving deltas:   8% (30/356)   \rResolving deltas:   9% (34/356)   \rResolving deltas:  10% (37/356)   \rResolving deltas:  11% (40/356)   \rResolving deltas:  12% (45/356)   \rResolving deltas:  13% (49/356)   \rResolving deltas:  17% (62/356)   \rResolving deltas:  19% (68/356)   \rResolving deltas:  20% (72/356)   \rResolving deltas:  21% (77/356)   \rResolving deltas:  25% (89/356)   \rResolving deltas:  28% (101/356)   \rResolving deltas:  29% (105/356)   \rResolving deltas:  44% (160/356)   \rResolving deltas:  49% (177/356)   \rResolving deltas:  50% (178/356)   \rResolving deltas:  55% (199/356)   \rResolving deltas:  57% (204/356)   \rResolving deltas:  59% (212/356)   \rResolving deltas:  62% (222/356)   \rResolving deltas:  68% (243/356)   \rResolving deltas:  70% (250/356)   \rResolving deltas:  71% (256/356)   \rResolving deltas:  72% (257/356)   \rResolving deltas:  75% (267/356)   \rResolving deltas:  76% (272/356)   \rResolving deltas:  79% (282/356)   \rResolving deltas:  80% (286/356)   \rResolving deltas:  82% (292/356)   \rResolving deltas:  83% (297/356)   \rResolving deltas:  85% (303/356)   \rResolving deltas:  87% (310/356)   \rResolving deltas:  91% (324/356)   \rResolving deltas:  92% (328/356)   \rResolving deltas:  95% (339/356)   \rResolving deltas:  96% (343/356)   \rResolving deltas:  97% (347/356)   \rResolving deltas:  98% (349/356)   \rResolving deltas:  99% (353/356)   \rResolving deltas: 100% (356/356)   \rResolving deltas: 100% (356/356), done.\n",
            "/content/tcav\n",
            " CONTRIBUTING.md\t LICENSE     requirements.txt   setup.py\n",
            " FetchDataAndModels.sh\t README.md  'Run TCAV.ipynb'    tcav\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xHiswgX0vy9Z",
        "colab_type": "code",
        "outputId": "9a998fe9-17ff-42c2-d937-861dce1109bf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 952
        }
      },
      "source": [
        "%cd /content/tcav/tcav/tcav_examples/image_models/imagenet\n",
        "%run download_and_make_datasets.py --source_dir=YOUR_FOLDER --number_of_images_per_folder=10 --number_of_random_folders=10"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/tcav/tcav/tcav_examples/image_models/imagenet\n",
            "Created source directory at YOUR_FOLDER\n",
            "WARNING:tensorflow:From /content/tcav/tcav/tcav_examples/image_models/imagenet/imagenet_and_broden_fetcher.py:163: The name tf.logging.info is deprecated. Please use tf.compat.v1.logging.info instead.\n",
            "\n",
            "INFO:tensorflow:Fetching imagenet data for zebra\n",
            "INFO:tensorflow:Saving images at YOUR_FOLDER/zebra\n",
            "INFO:tensorflow:Problem downloading imagenet image. Exception was HTTP Error 404: Not Found for URL http://www.featurepics.com/FI/Marked/20060909/Zebra84929.jpg\n",
            "\n",
            "INFO:tensorflow:Problem downloading imagenet image. Exception was <urlopen error [Errno -2] Name or service not known> for URL http://bonfire.learnnc.org/zoo/week02/photos/mccrary_zootales/images/zebra.jpg\n",
            "\n",
            "INFO:tensorflow:Problem downloading imagenet image. Exception was HTTP Error 404: Not Found for URL http://www.istockphoto.com/file_thumbview_approve/269671/2/istockphoto_269671_zebra.jpg\n",
            "\n",
            "INFO:tensorflow:Problem downloading imagenet image. Exception was HTTP Error 410: Gone for URL http://www.mediastorehouse.com/image/Grevys-Zebra_463879.jpg\n",
            "\n",
            "INFO:tensorflow:Problem downloading imagenet image. Exception was HTTP Error 404: Not Found for URL http://www.lorilamont.com/Zebra.jpg\n",
            "\n",
            "INFO:tensorflow:Problem downloading imagenet image. Exception was HTTP Error 404: Not Found for URL http://www.cathouse-fcc.org/gifs-jpegs/southafrica/zebra.jpg\n",
            "\n",
            "INFO:tensorflow:Problem downloading imagenet image. Exception was <urlopen error [Errno -3] Temporary failure in name resolution> for URL http://www.kilimanjaro.com/animals/zebra.jpg\n",
            "\n",
            "INFO:tensorflow:Problem downloading imagenet image. Exception was <urlopen error [Errno -2] Name or service not known> for URL http://www.conceptexpeditions.com/images/zebra.jpg\n",
            "\n",
            "INFO:tensorflow:Problem downloading imagenet image. Exception was HTTP Error 404: Not Found for URL http://alumnus.caltech.edu/~kantner/zebras/pictures/zebra.jpg\n",
            "\n",
            "INFO:tensorflow:Problem downloading imagenet image. Exception was HTTP Error 403: Forbidden for URL http://www.zslprints.com/image/Chapmans-Zebra_561562.jpg\n",
            "\n",
            "INFO:tensorflow:Problem downloading imagenet image. Exception was [Errno 104] Connection reset by peer for URL http://www.free-slideshow.com/stock-photos/lovely_animals/zebra.jpg\n",
            "\n",
            "INFO:tensorflow:Problem downloading imagenet image. Exception was <urlopen error [Errno -2] Name or service not known> for URL http://www.namibia-lov.com/lovnadivljac/zebra0001p4.jpg\n",
            "\n",
            "INFO:tensorflow:Problem downloading imagenet image. Exception was HTTP Error 404: Not Found for URL http://www.hoopoe.com/images/lioneatingzebra.jpg\n",
            "\n",
            "INFO:tensorflow:Problem downloading imagenet image. Exception was HTTP Error 404: Not found for URL http://www.northrup.org/Photos/zebra/low/baby-zebra-with-mother.jpg\n",
            "\n",
            "INFO:tensorflow:Problem downloading imagenet image. Exception was HTTP Error 404: Not Found for URL http://www.kotaku.com/assets/resources/2006/09/afrika_zebra_sm.jpg\n",
            "\n",
            "Downloaded 10 for zebra\n",
            "INFO:tensorflow:Using path YOUR_FOLDER/broden1_224/broden1_224/images/dtd/ for texture: striped\n",
            "INFO:tensorflow:We have 120 images for the concept striped\n",
            "INFO:tensorflow:Using path YOUR_FOLDER/broden1_224/broden1_224/images/dtd/ for texture: dotted\n",
            "INFO:tensorflow:We have 120 images for the concept dotted\n",
            "INFO:tensorflow:Using path YOUR_FOLDER/broden1_224/broden1_224/images/dtd/ for texture: zigzagged\n",
            "INFO:tensorflow:We have 120 images for the concept zigzagged\n",
            "INFO:tensorflow:Downloaded 10/10 for random500_0\n",
            "INFO:tensorflow:Downloaded 10/10 for random500_1\n",
            "INFO:tensorflow:Downloaded 10/10 for random500_2\n",
            "INFO:tensorflow:Downloaded 10/10 for random500_3\n",
            "INFO:tensorflow:Downloaded 10/10 for random500_4\n",
            "INFO:tensorflow:Downloaded 10/10 for random500_5\n",
            "INFO:tensorflow:Downloaded 10/10 for random500_6\n",
            "INFO:tensorflow:Downloaded 10/10 for random500_7\n",
            "INFO:tensorflow:Downloaded 10/10 for random500_8\n",
            "INFO:tensorflow:Downloaded 10/10 for random500_9\n",
            "INFO:tensorflow:Downloaded 10/10 for random500_10\n",
            "Successfully created data at YOUR_FOLDER\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lhRgRiO_Zpu0",
        "colab_type": "code",
        "outputId": "dac68471-b71f-4263-f3ea-b00c1295a12b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "%cd /content/tcav"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/tcav\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ij_QWVD5FzIk",
        "colab_type": "text"
      },
      "source": [
        "# Running TCAV\n",
        "\n",
        "\n",
        "This notebook walks you through things you need to run TCAV. \n",
        "\n",
        "Before running this notebook, run the following to download all the data.\n",
        "\n",
        "```\n",
        "cd tcav/tcav_examples/image_models/imagenet\n",
        "\n",
        "python download_and_make_datasets.py --source_dir=YOUR_PATH --number_of_images_per_folder=50 --number_of_random_folders=3\n",
        "```\n",
        "\n",
        "In high level, you need:\n",
        "\n",
        "1. **example images in each folder** (you have this if you ran the above)\n",
        " * images for each concept\n",
        " * images for the class/labels of interest\n",
        " * random images that will be negative examples when learning CAVs (images that probably don't belong to any concepts)\n",
        "2. **model wrapper** (below uses example from tcav/model.py)\n",
        " * an instance of  ModelWrapper abstract class (in model.py). This tells TCAV class (tcav.py) how to communicate with your model (e.g., getting internal tensors)\n",
        "3. **act_generator** (below uses example from tcav/activation_generator.py)\n",
        " * an instance of ActivationGeneratorInterface that tells TCAV class how to load example data and how to get activations from the model\n",
        "\n",
        "\n",
        "## Requirements\n",
        "\n",
        "    pip install the tcav and tensorflow packages (or tensorflow-gpu if using GPU)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g8m6xBvYFzIm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RQ0iGRjNFzIt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tcav.activation_generator as act_gen\n",
        "import tcav.cav as cav\n",
        "import tcav.model  as model\n",
        "import tcav.tcav as tcav\n",
        "import tcav.utils as utils\n",
        "import tcav.utils_plot as utils_plot # utils_plot requires matplotlib\n",
        "import os \n",
        "import tensorflow as tf"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VhThYR_EFzIx",
        "colab_type": "text"
      },
      "source": [
        "## Step 1. Store concept and target class images to local folders\n",
        "\n",
        "and tell TCAV where they are.\n",
        "\n",
        "**source_dir**: where images of concepts, target class and random images (negative samples when learning CAVs) live. Each should be a sub-folder within this directory.\n",
        "\n",
        "Note that random image directories can be in any name. In this example, we are using `random500_0`, `random500_1`,.. for an arbitrary reason. \n",
        "\n",
        "You need roughly 50-200 images per concept and target class (10-20 pictures also tend to work, but 200 is pretty safe).\n",
        "\n",
        "\n",
        "**cav_dir**: directory to store CAVs (`None` if you don't want to store)\n",
        "\n",
        "**target, concept**: names of the target class (that you want to investigate) and concepts (strings) - these are folder names in source_dir\n",
        "\n",
        "**bottlenecks**: list of bottleneck names (intermediate layers in your model) that you want to use for TCAV. These names are defined in the model wrapper below.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lPX3LOiZFzIy",
        "colab_type": "code",
        "outputId": "b3fe647f-c595-4f7f-b9d9-fbf6496dfe18",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# This is the name of your model wrapper (InceptionV3 and GoogleNet are provided in model.py)\n",
        "model_to_run = 'GoogleNet'\n",
        "# the name of the parent directory that results are stored (only if you want to cache)\n",
        "project_name = 'tcav_class_test'\n",
        "working_dir = '/content/tcav/tcav'\n",
        "# where activations are stored (only if your act_gen_wrapper does so)\n",
        "activation_dir =  working_dir+ '/activations/'\n",
        "# where CAVs are stored. \n",
        "# You can say None if you don't wish to store any.\n",
        "cav_dir = working_dir + '/cavs/'\n",
        "# where the images live. \n",
        "source_dir = '/content/tcav/tcav/tcav_examples/image_models/imagenet/YOUR_FOLDER'\n",
        "bottlenecks = [ 'mixed4c']  # @param \n",
        "      \n",
        "utils.make_dir_if_not_exists(activation_dir)\n",
        "utils.make_dir_if_not_exists(working_dir)\n",
        "utils.make_dir_if_not_exists(cav_dir)\n",
        "\n",
        "# this is a regularizer penalty parameter for linear classifier to get CAVs. \n",
        "alphas = [0.1]   \n",
        "\n",
        "target = 'zebra'  \n",
        "concepts = [\"dotted\",\"striped\",\"zigzagged\"]   \n"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "REMEMBER TO UPDATE YOUR_PATH (where images, models are)!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vmdo01TEFzI4",
        "colab_type": "text"
      },
      "source": [
        "## Step 2. Write your model wrapper\n",
        "\n",
        "Next step is to tell TCAV how to communicate with your model. See `model.GoogleNetWrapper_public ` for details.\n",
        "\n",
        "You can define a subclass of ModelWrapper abstract class to do this. Let me walk you thru what each function does (tho they are pretty self-explanatory).  This wrapper includes a lot of the functions that you already have, for example, `get_prediction`.\n",
        "\n",
        "### 1. Tensors from the graph: bottleneck tensors and ends\n",
        "First, store your bottleneck tensors in `self.bottlenecks_tensors` as a dictionary. You only need bottlenecks that you are interested in running TCAV with. Similarly, fill in `self.ends` dictionary with `input`, `logit` and `prediction` tensors.\n",
        "\n",
        "### 2. Define loss\n",
        "Get your loss tensor, and assigned it to `self.loss`. This is what TCAV uses to take directional derivatives. \n",
        "\n",
        "While doing so, you would also want to set \n",
        "```python\n",
        "self.y_input \n",
        "```\n",
        "this simply is a tensorflow place holder for the target index in the logit layer (e.g., 0 index for a dog, 1 for a cat).\n",
        "For multi-class classification, typically something like this works:\n",
        "\n",
        "```python\n",
        "self.y_input = tf.placeholder(tf.int64, shape=[None])\n",
        "```\n",
        "\n",
        "For example, for a multiclass classifier, something like below would work. \n",
        "\n",
        "```python\n",
        "    # Construct gradient ops.\n",
        "    with g.as_default():\n",
        "      self.y_input = tf.placeholder(tf.int64, shape=[None])\n",
        "\n",
        "      self.pred = tf.expand_dims(self.ends['prediction'][0], 0)\n",
        "\n",
        "      self.loss = tf.reduce_mean(\n",
        "          tf.nn.softmax_cross_entropy_with_logits(\n",
        "              labels=tf.one_hot(self.y_input, len(self.labels)),\n",
        "              logits=self.pred))\n",
        "    self._make_gradient_tensors()\n",
        "```\n",
        "\n",
        "### 3. Call _make_gradient_tensors in __init__() of your wrapper\n",
        "```python\n",
        "_make_gradient_tensors()  \n",
        "```\n",
        "does what you expect - given the loss and bottleneck tensors defined above, it adds gradient tensors.\n",
        "\n",
        "### 4. Fill in labels, image shapes and a model name.\n",
        "Get the mapping from labels (strings) to indice in the logit layer (int) in a dictionary format.\n",
        "\n",
        "```python\n",
        "def id_to_label(self, idx)\n",
        "def label_to_id(self, label)\n",
        "```\n",
        "\n",
        "Set your input image shape at  `self.image_shape`\n",
        "\n",
        "\n",
        "Set your model name to `self.model_name`\n",
        "\n",
        "You are done with writing the model wrapper! I wrote two model wrapers, InceptionV3 and Googlenet."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZvEO4u1iFzI4",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "**sess**: a tensorflow session."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KmV0F2bQdHpq",
        "colab_type": "code",
        "outputId": "812a6ad5-69be-4018-827f-eaf029f05924",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        }
      },
      "source": [
        "%cp -av '/content/tcav/tcav/tcav_examples/image_models/imagenet/YOUR_FOLDER/mobilenet_v2_1.0_224' '/content/tcav/tcav/mobilenet_v2_1.0_224'\n",
        "%rm '/content/tcav/tcav/tcav_examples/image_models/imagenet/YOUR_FOLDER/mobilenet_v2_1.0_224'"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "'/content/tcav/tcav/tcav_examples/image_models/imagenet/YOUR_FOLDER/mobilenet_v2_1.0_224' -> '/content/tcav/tcav/mobilenet_v2_1.0_224'\n",
            "'/content/tcav/tcav/tcav_examples/image_models/imagenet/YOUR_FOLDER/mobilenet_v2_1.0_224/mobilenet_v2_1.0_224_eval.pbtxt' -> '/content/tcav/tcav/mobilenet_v2_1.0_224/mobilenet_v2_1.0_224_eval.pbtxt'\n",
            "'/content/tcav/tcav/tcav_examples/image_models/imagenet/YOUR_FOLDER/mobilenet_v2_1.0_224/mobilenet_v2_1.0_224.ckpt.data-00000-of-00001' -> '/content/tcav/tcav/mobilenet_v2_1.0_224/mobilenet_v2_1.0_224.ckpt.data-00000-of-00001'\n",
            "'/content/tcav/tcav/tcav_examples/image_models/imagenet/YOUR_FOLDER/mobilenet_v2_1.0_224/mobilenet_v2_1.0_224_frozen.pb' -> '/content/tcav/tcav/mobilenet_v2_1.0_224/mobilenet_v2_1.0_224_frozen.pb'\n",
            "'/content/tcav/tcav/tcav_examples/image_models/imagenet/YOUR_FOLDER/mobilenet_v2_1.0_224/mobilenet_v2_1.0_224.tflite' -> '/content/tcav/tcav/mobilenet_v2_1.0_224/mobilenet_v2_1.0_224.tflite'\n",
            "'/content/tcav/tcav/tcav_examples/image_models/imagenet/YOUR_FOLDER/mobilenet_v2_1.0_224/mobilenet_v2_1.0_224_info.txt' -> '/content/tcav/tcav/mobilenet_v2_1.0_224/mobilenet_v2_1.0_224_info.txt'\n",
            "'/content/tcav/tcav/tcav_examples/image_models/imagenet/YOUR_FOLDER/mobilenet_v2_1.0_224/mobilenet_v2_1.0_224.ckpt.meta' -> '/content/tcav/tcav/mobilenet_v2_1.0_224/mobilenet_v2_1.0_224.ckpt.meta'\n",
            "'/content/tcav/tcav/tcav_examples/image_models/imagenet/YOUR_FOLDER/mobilenet_v2_1.0_224/mobilenet_v2_1.0_224.ckpt.index' -> '/content/tcav/tcav/mobilenet_v2_1.0_224/mobilenet_v2_1.0_224.ckpt.index'\n",
            "rm: cannot remove '/content/tcav/tcav/tcav_examples/image_models/imagenet/YOUR_FOLDER/mobilenet_v2_1.0_224': Is a directory\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C3Gf9yAag2JM",
        "colab_type": "code",
        "outputId": "742b37ff-fca6-4138-a306-24e0db1b26ff",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "%cp -av '/content/tcav/tcav/tcav_examples/image_models/imagenet/YOUR_FOLDER/inception5h' '/content/tcav/tcav/inception5h'\n",
        "%rm '/content/tcav/tcav/tcav_examples/image_models/imagenet/YOUR_FOLDER/inception5h'"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "'/content/tcav/tcav/tcav_examples/image_models/imagenet/YOUR_FOLDER/inception5h' -> '/content/tcav/tcav/inception5h'\n",
            "'/content/tcav/tcav/tcav_examples/image_models/imagenet/YOUR_FOLDER/inception5h/imagenet_comp_graph_label_strings.txt' -> '/content/tcav/tcav/inception5h/imagenet_comp_graph_label_strings.txt'\n",
            "'/content/tcav/tcav/tcav_examples/image_models/imagenet/YOUR_FOLDER/inception5h/tensorflow_inception_graph.pb' -> '/content/tcav/tcav/inception5h/tensorflow_inception_graph.pb'\n",
            "'/content/tcav/tcav/tcav_examples/image_models/imagenet/YOUR_FOLDER/inception5h/LICENSE' -> '/content/tcav/tcav/inception5h/LICENSE'\n",
            "rm: cannot remove '/content/tcav/tcav/tcav_examples/image_models/imagenet/YOUR_FOLDER/inception5h': Is a directory\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AqLdl3GBFzI5",
        "colab_type": "code",
        "outputId": "7fd0e23d-3d6a-43cd-9dad-df1189554eb3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        }
      },
      "source": [
        "sess = utils.create_session()\n",
        "\n",
        "# GRAPH_PATH is where the trained model is stored.\n",
        "GRAPH_PATH = \"/content/tcav/tcav/inception5h/tensorflow_inception_graph.pb\"\n",
        "# LABEL_PATH is where the labels are stored. Each line contains one class, and they are ordered with respect to their index in \n",
        "# the logit layer. (yes, id_to_label function in the model wrapper reads from this file.)\n",
        "# For example, imagenet_comp_graph_label_strings.txt looks like:\n",
        "# dummy                                                                                      \n",
        "# kit fox\n",
        "# English setter\n",
        "# Siberian husky ...\n",
        "\n",
        "LABEL_PATH = \"/content/tcav/tcav/inception5h/imagenet_comp_graph_label_strings.txt\"\n",
        "\n",
        "mymodel = model.GoogleNetWrapper_public(sess,\n",
        "                                        GRAPH_PATH,\n",
        "                                        LABEL_PATH)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /content/tcav/tcav/utils.py:40: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/tcav/tcav/utils.py:44: The name tf.InteractiveSession is deprecated. Please use tf.compat.v1.InteractiveSession instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/tcav/tcav/model.py:304: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/tcav/tcav/model.py:310: The name tf.GraphDef is deprecated. Please use tf.compat.v1.GraphDef instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/tcav/tcav/model.py:293: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/tcav/tcav/model.py:263: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "\n",
            "Future major versions of TensorFlow will allow gradients to flow\n",
            "into the labels input on backprop by default.\n",
            "\n",
            "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WlGsrlh3FzI-",
        "colab_type": "text"
      },
      "source": [
        "## Step 3. Implement a class that returns activations (maybe with caching!)\n",
        "\n",
        "Lastly, you will implement a class of the ActivationGenerationInterface which TCAV uses to load example data for a given concept or target, call into your model wrapper and return activations. I pulled out this logic outside of mymodel because this step often takes the longest. By making it modular, you can cache your activations and/or parallelize your computations, as I have done in `ActivationGeneratorBase.process_and_load_activations` in `activation_generator.py`.\n",
        "\n",
        "\n",
        "The `process_and_load_activations` method of the activation generator must return a dictionary of activations that has concept or target name as  a first key, and the bottleneck name as a second key. So something like:\n",
        "\n",
        "```python\n",
        "{concept1: {bottleneck1: [[0.2, 0.1, ....]]},\n",
        "concept2: {bottleneck1: [[0.1, 0.02, ....]]},\n",
        "target1: {bottleneck1: [[0.02, 0.99, ....]]}\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xIC73E-jFzI_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "act_generator = act_gen.ImageActivationGenerator(mymodel, source_dir, activation_dir, max_examples=100)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oj-b7hEUFzJD",
        "colab_type": "text"
      },
      "source": [
        "## You are ready to run TCAV!\n",
        "\n",
        "Let's do it.\n",
        "\n",
        "**num_random_exp**: number of experiments to confirm meaningful concept direction. TCAV will search for this many folders named `random500_0`, `random500_1`, etc. You can alternatively set the `random_concepts` keyword to be a list of folders of random concepts. Run at least 10-20 for meaningful tests. \n",
        "\n",
        "**random_counterpart**: as well as the above, you can optionally supply a single folder with random images as the \"positive set\" for statistical testing. Reduces computation time at the cost of less reliable random TCAV scores. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "J4SQ49frFzJE",
        "colab_type": "code",
        "outputId": "3c1c21a8-615b-4a44-cb05-64782862a0fe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "source": [
        "tf.logging.set_verbosity(0)\n",
        "num_random_exp=10\n",
        "## only running num_random_exp = 10 to save some time. The paper number are reported for 500 random runs. \n",
        "mytcav = tcav.TCAV(sess,\n",
        "                   target,\n",
        "                   concepts,\n",
        "                   bottlenecks,\n",
        "                   act_generator,\n",
        "                   alphas,\n",
        "                   cav_dir=cav_dir,\n",
        "                   num_random_exp=num_random_exp)#10)\n",
        "print ('This may take a while... Go get coffee!')\n",
        "results = mytcav.run(run_parallel=False)\n",
        "print ('done!')"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "This may take a while... Go get coffee!\n",
            "WARNING:tensorflow:\n",
            "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "  * https://github.com/tensorflow/io (for I/O related ops)\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n",
            "done!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "3fD-6zdqFzJN",
        "colab_type": "code",
        "outputId": "e2a2f93c-6b82-4112-f838-9745a127e627",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 433
        }
      },
      "source": [
        "utils_plot.plot_results(results, num_random_exp=num_random_exp)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Class = zebra\n",
            "  Concept = dotted\n",
            "    Bottleneck = mixed4c. TCAV Score = 0.49 (+- 0.21), random was 0.48 (+- 0.22). p-val = 0.903 (not significant)\n",
            "  Concept = striped\n",
            "    Bottleneck = mixed4c. TCAV Score = 0.86 (+- 0.10), random was 0.48 (+- 0.22). p-val = 0.000 (significant)\n",
            "  Concept = zigzagged\n",
            "    Bottleneck = mixed4c. TCAV Score = 0.88 (+- 0.15), random was 0.48 (+- 0.22). p-val = 0.000 (significant)\n",
            "{'mixed4c': {'bn_vals': [0.01, 0.8600000000000001, 0.8800000000000001], 'bn_stds': [0, 0.10198039027185571, 0.14696938456699069], 'significant': [False, True, True]}}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAacAAAEYCAYAAAD4czk4AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+j8jraAAAgAElEQVR4nO3de7wVdb3/8dcbAREllYslgoKG4oWj0c7LDz1Rage1oHPSDoQVZnDqSGVpap2TmVkPLbGLaUqmaCWKHU1UvKFyvJQKKmoiEgnqRjFALdCDF/z8/pjv1mGx12Jt3Wuv2az38/FYjz3zne985zOzZs9nzcx3zVJEYGZmViRd6h2AmZlZKScnMzMrHCcnMzMrHCcnMzMrHCcnMzMrHCcnMzMrHCcnKzRJW0i6TtLfJV1V73jKkbRU0iH1jqOzk3SapN+WmTZSUnONl1/X91HSNEln1Gv5ReLk1AEkrcm93pT0f7nx8ZLeI+mnkp5OZX9N431L2pkj6UVJm6fx/SW9LGmrVpb5kKTJZeL5tqQlaVnNkq6szZq3iyOB9wJ9IuKoegfTaColi85E0gRJd5eUOREUmJNTB4iIrVpewNPAJ3LjVwG3AXsCo4D3AAcAq4B9W9qQNAg4CAhgdGr3XqCZ7ABOru5ewB7A9NJYJH0e+CxwSFp+U1p+u5HUtR2b2wlYFBFv1DkOM+tIEeFXB76ApWSJoWX8i8DzwFYbme9U4B7gHOD6XPm3gdtL6v4IuKZMO78AflphOb2BS4BngReBP+SmTQQWAy8AM4H+uWkBHAf8BViSyj4OzAdeAv4I/FOu/snAMmA18ARwcCuxfA94DXgdWAMcS/aB6r+Bp4C/AZcBW6f6g1Icx5J9CLizzDpWiusU4K8prgXAv5bMOxF4PDd9eO59PRF4BPg7cCXQo8J2LtfO7sCcFNtjwOjcPNOA84Ab0nz3Abvkpu8J3Jren+eBb6fyLrn1WgXMAHqXbLNJ6T1/DjgxTRtVsv0fLrMuZbcZMAG4GzibbH9aAhyWmz4Y+N80761k++dvyyxnJNmHsW8DK9M2H5+bvnXaH1ak/eO/07rvDqwF1qX1eCmt7+tp/dYA15X+f1a53T5Ptq+tBP4rF0vZedP0A8n2vZeAZ4AJuff4jDTcC7gD+Dmgeh+7OvpV9wAa7cWGyekK4NIq5lsM/CfwwfRP9d5UPhB4AxiYxrukf+BPlmnnaLKD1zfJzpo2K5l+A9mBdVugG/DhVP7R9A84HNgcOJfcwT/9o95Klty2AD5Aljz2AzZL/8RL07y7pX/I/mneQeQOsiXxnEbuYAV8IW2LnYGtgKuB3+TaCbID1JbAFq20VzauNP0ooH/ajv8OvAxsn5u2DPgQIOD9wE659/X+NG9vssTzpTLr1Go7aXsvJjv4dk/bfDWwW5pvGm+fUXcFfgdckab1IkssJwA90vh+adrXgHuBAWn7XwhML9lm09M2G0Z2cD+kte1fYX3KbbMJZPvrxLS9v0yWBJWm/4nsA9fmwD+n9a2UnN7I1f9wWlbL9rkMuDat+yBgEXBsLo67S9qbRkoErf1/VrndfkW2v+8NvArsXsW8O6X1HJfe8z7APvmYUtn9pfE10qvuATTaiw2T063AmRuZ58D0D943jS8Evp6bPpu3PyUfmg4u3Sq0Nz7N8zLZwe7kVL498CawbSvz/Br4UW58qxTToDQewEdz038JfL+kjSfSAeX9ZAnikEpxpnlOY/3kdBvwn7nx3VIcXXMHjJ0rtFc2rjL15wNj0vDNwNcqvK9H58Z/BFxQpm6r7ZBdtl0OdMmVTQdOS8PTgIty0w4HFqbhccBDZZb3OLkz0/Q+l26zoSWx/7q17V/lPp7fZhOAxblpPdPy3gfsSJZstsxNv7zc8ng7OeXrzwC+Q5b4XgP2yE37D2BOLo62JqdqttuA3PT7gbFVzPstyl/ZmAZcDPwZ+GZbtvum9vI9p/pbRbbjVvJ54JaIWJnGL09lLS4lu49E+ntFRLxerrGI+F1EHAJsA3wJ+L6kfyE7C3shIl5sZbb+ZJdKWtpYk2LfIVfnmdzwTsAJkl5qeaX2+0fEYuB4sgPf3yRdIal/xS1QJo403JWs00RrcZQqGxeApM9Jmp+bthfQ0jFlINllmnKW54ZfIUvgrSnXTn/gmYh4M1f2FOtv43LLqBTbTsA1uXV6nOwSV7lt9lSKpSob2WbrxRwRr6TBrdIyXoyIl0uWXUlr9fun5XVjw30jv+3aqprtVu79qDTvxvajI8jOxi54F7F3ek5O9Tcb+BdJW7Y2UdIWwKeBD0taLmk58HVgb0l7p2pXAwMkfQT4N7JktVER8XpEXEV2n2QvsgNUb0nbtFL9WbJ/uJa4tiS79LAs32Ru+BngBxGxTe7VMyKmp2VfHhEHpjYDOKuamEvj4O1P38+XiaNU2bgk7UR2mWYyWe/Abcg+wSo37y5VxllJuXaeBQZKyv9f7sj627hSmztXmHZYyTr3iIh8uwNLlvlsGq60Lalim1XyHLBtyb6/40bmaa3+s2SXnF9nw32jZR1bW4+K60Z12+2dzLux/ehXwE3ArHLHhUbg5FR/vyHbWf9H0lBJXST1Sd29Dwc+SfaJaw9gn/TaHbgL+BxA+iT5e7KODE9FxLxyC0tdao+Q1Cst6zCyG+n3RcRzwI3A+ZK2ldRN0j+nWacDx0jaJ3Vl/2GaZ2mZRf0K+JKk/ZTZMrfc3SR9NLWzFvg/ssuJ1ZgOfF3S4NSF/ofAlVF9b76ycZHdcwmyy6JIOoYsabe4CDhR0gfTvO9PB+e2KtfOfWSfvk9K234k8Amy+5Ibcz2wvaTjJW2etvN+adoFwA9aYpXUT9KYkvm/I6mnpD2BY8juO0KW9AeVJMy8jW2zsiLiKWAe8D1J3SUdmNZ3Y1rqH0TWueWqiFhHdonvB2nddwK+AbR0g3+e7ANc91w7z1M+oUN12+2dzPs74BBJn5bUNf2/71My/2Syy83XpQ+oDcfJqc4i4lWyey8Lye4//YPs2nVfsoPV54FLIuLpiFje8iLr1TQ+1136UrJPjZdtZJH/ILvh/jRZT6EfAV+OiJbvgHyW7BPoQrL7QsenOGeTXdv/H7JPvLsAYyus1zyym+C/IOultZjsuj9kN4jPJPu0uxzYjuw6fDUuJkvod5L1/FoLfKXKeSvGFRELgClkN+mfJ+sccE9u3quAH5BdVl0N/IGs80OblGsnIl4jOzgfRrZtzgc+FxELq2hzNdn9xk+QbdO/AB9Jk39G1rvyFkmryW7U71fSxP+SbYvbgLMj4pZU3vLF51WSHmxluRW3WRU+k2J5AfguG99/l5O9b8+SHeS/lNs+XyG7j/okWQ/By8n2F4DbyXo/LpfUcnn818Ae6dLbH1pZVjXbrZyy80bE02T3C09I6z2frEPFWyIiyHoUNgPXSupR5XI3GS09ZsysASn7/twSso4pbf4umVmt+MzJzMwKx8nJzMwKx5f1zMyscHzmZGZmhdPpHozZt2/fGDRoUL3DMDOzCh544IGVEdHvnc7f6ZLToEGDmDev7Nd4zMysACRt7GkfFfmynpmZFY6Tk5mZFY6Tk5mZFU6nu+dkZlYrr7/+Os3Nzaxdu7beoXQaPXr0YMCAAXTr1q1d23VyMjNLmpub6dWrF4MGDUKq5sHqjS0iWLVqFc3NzQwePLhd2/ZlPTOzZO3atfTp08eJqUqS6NOnT03ONJ2czMxynJjaplbby8nJzMwKx/eczMzKGHTKDe3a3tIzj2iXdmbOnMmCBQs45ZRT3lU7c+bM4eyzz+b6669/q2zu3LkccMABXHHFFRx55JHvNtR3zMnJzKyTGT16NKNHj273dtetW8fJJ5/Mxz72sXZvu618Wc/MrEBuu+02dt55ZyZMmMCuu+7K+PHjmT17NiNGjGDIkCHcf//9TJs2jcmTJwMwZswYLrss+wHhCy+8kPHjxwNwyy23cMABBzB8+HCOOuoo1qxZA8BNN93E0KFDGT58OFdfffV6yz733HP51Kc+xXbbbbde+VlnncWwYcPYe++93/XZWrWcnMzMCubpp5/mhBNOYOHChSxcuJDLL7+cu+++m7PPPpsf/vCH69WdOnUqp59+OnfddRdTpkzh3HPPZeXKlZxxxhnMnj2bBx98kKamJs455xzWrl3LxIkTue6663jggQdYvnz5W+0sW7aMa665hi9/+cvrtX/jjTdy7bXXct999/Hwww9z0kkndcg2qFlyknSxpL9J+nOZ6ZL0c0mLJT0iaXitYjEz60wGDBjAsGHD6NKlC3vuuScHH3wwkhg2bBhLly5dr+573/teTj/9dD7ykY8wZcoUevfuzb333suCBQsYMWIE++yzD5deeilPPfUUCxcuZPDgwQwZMgRJHH300W+1c/zxx3PWWWfRpcv6aWH27Nkcc8wx9OzZE4DevXvXfP2htvecpgG/AC4rM/0wYEh67Qf8Mv01M2to3bt3f2u4S5cubL755m8Nv/HGGxvUf/TRR+nTpw/PPvsskH059tBDD2X69Onr1Zs/f37ZZc6bN4+xY8cCsHLlSmbNmkXXrvXrllCzM6eIuBN4oUKVMcBlkbkX2EbS9rWKx8xsU3T//fdz44038tBDD3H22WezZMkS9t9/f+655x4WL14MwMsvv8yiRYsYOnQoS5cu5a9//SvAeslryZIlLF26lKVLl3LkkUdy/vnn88lPfpJDDz2USy65hFdeeQWAF16odFhvP/XsrbcD8ExuvDmVPVdaUdIkYBLAjjvu2CHBmZm1V9fvWnn11VeZOHEil1xyCf3792fKlCl84Qtf4Pbbb2fatGmMGzeOV199FYAzzjiDXXfdlalTp3LEEUfQs2dPDjroIFavXl1xGaNGjWL+/Pk0NTXRvXt3Dj/88A3ue9WCIqJ2jUuDgOsjYq9Wpl0PnBkRd6fx24CTI6LiLwk2NTWFf2zQzGrh8ccfZ/fdd69rDE888QQAu+22W13jaIvWtpukByKi6Z22Wc/eesuAgbnxAanMzMwaXD2T00zgc6nX3v7A3yNig0t6ZmbWeGp2z0nSdGAk0FdSM/BdoBtARFwAzAIOBxYDrwDH1CoWM7NqRYQf/toGtbo1VLPkFBHjNjI9gONqtXwzs7bq0aMHq1at8s9mVKnl95x69OjR7m372XpmZsmAAQNobm5mxYoVdYuh5akNb775Zt1iaIuWX8Jtb05OZmZJt27d2v0XXduq5fFBc+bMqWsc9eZn65mZWeE4OZnV2ciRIxk5cmS9wzArFCcnMzMrHCcnMzMrHCcnMzMrHCcnMzMrHCcnMzMrHCcnMzMrHCcnMzMrHCcnMzMrHCcnMzMrHCcnMzMrHCcnMzMrHCcnMzMrHCcnMzMrHCcnMzMrHCcnMzMrHCcnMzMrHP9Mu5lZzqBTbqjr8pc/uaoQcSw984i6Lt9nTmZmVjhOTmZmVjhOTmZmVjhOTmZmVjjuEGENr943nn0D3GxDPnMyM7PCcXIyM7PCcXIyM7PCcXIyM7PCcXIyM7PCcXIyM7PCcXIyM7PCcXIyM7PCqWlykjRK0hOSFks6pZXpO0q6Q9JDkh6RdHgt4zEzs86hZslJ0mbAecBhwB7AOEl7lFT7b2BGRHwAGAucX6t4zMys86jlmdO+wOKIeDIiXgOuAMaU1AngPWl4a+DZGsZjZmadRC2frbcD8ExuvBnYr6TOacAtkr4CbAkcUsN4zMysk6h3h4hxwLSIGAAcDvxG0gYxSZokaZ6keStWrOjwIM3MrGPVMjktAwbmxgeksrxjgRkAEfEnoAfQt7ShiJgaEU0R0dSvX78ahWtmZkVRy+Q0FxgiabCk7mQdHmaW1HkaOBhA0u5kycmnRmZmDa5mySki3gAmAzcDj5P1yntM0umSRqdqJwATJT0MTAcmRETUKiYzM+scavpjgxExC5hVUnZqbngBMKKWMZiZWedT7w4RZmZmG3ByMjOzwnFyMjOzwnFyMjOzwqlphwgz27j3febMeodgVjg+czIzs8JxcjIzs8JxcjIzs8JxcjIzs8JxhwgzswJxB5mMz5zMzKxwnJzMzKxwnJzMzKxwnJzMzKxwnJzMzKxwnJzMzKxwnJzMzKxwnJzMzKxwnJzMzKxwnJzMzKxwnJzMzKxwnJzMzKxwnJzMzKxwnJzMzKxwnJzMzKxwNpqcJPWU9B1Jv0rjQyR9vPahmZlZo6rmzOkS4FXggDS+DDijZhGZmVnDqyY57RIRPwJeB4iIVwDVNCozM2to1SSn1yRtAQSApF3IzqTMzMxqomsVdb4L3AQMlPQ7YAQwoZZBmZlZY6uYnCR1AbYF/g3Yn+xy3tciYmUHxGZmZg2qYnKKiDclnRQRM4AbOigmMzNrcNXcc5ot6URJAyX1bnnVPDIzM2tY1dxz+vf097hcWQA7t384ZmZmVSSniBjcEYGYmZm1qOYJEd0kfVXS79NrsqRu1TQuaZSkJyQtlnRKmTqflrRA0mOSLm/rCpiZ2aanmst6vwS6Aeen8c+msi9WmknSZsB5wKFAMzBX0syIWJCrMwT4FjAiIl6UtF3bV8HMzDY11SSnD0XE3rnx2yU9XMV8+wKLI+JJAElXAGOABbk6E4HzIuJFgIj4W3Vhm5nZpqya3nrr0lMhAJC0M7Cuivl2AJ7JjTensrxdgV0l3SPpXkmjWmtI0iRJ8yTNW7FiRRWLNjOzzqyaM6dvAndIepLsS7g7Ace04/KHACOBAcCdkoZFxEv5ShExFZgK0NTUFO20bDMzK6hqeuvdlu4N7ZaKnoiIap6ttwwYmBsfkMrymoH7IuJ1YImkRWTJam4V7ZuZ2Saqmt56xwFbRMQjEfEI0FPSf1bR9lxgiKTBkroDY4GZJXX+QHbWhKS+ZJf5nmxD/GZmtgmq5p7TxPxlttR5YeLGZoqIN4DJwM3A48CMiHhM0umSRqdqNwOrJC0A7gC+GRGr2roSZma2aanmntNmkhQRLT+ZsRnQvZrGI2IWMKuk7NTccADfSC8zMzOguuR0E3ClpAvT+H+kMjMzs5qoJjmdDEwCvpzGbwUuqllEZmbW8KrprfcmcIGki4E9gWURUc33nMzMzN6Rsh0iJF0gac80vDUwH7gMeEjSuA6Kz8zMGlCl3noHRcRjafgYYFFEDAM+CJxU88jMzKxhVUpOr+WGDyX7ThIRsbymEZmZWcOrlJxekvRxSR8ARpB66EnqCmzREcGZmVljqtQh4j+AnwPvA47PnTEdDNxQ68DMzKxxlU1OEbEI2OAp4RFxM9mTHczMzGqimscXmZmZdSgnJzMzK5xK33N6X0cGYmZm1qLSmdN8SbMlHStpmw6LyMzMGl6l5LQD8GPgQOAJSddKGivJ3cjNzKymyianiFgXETdHxDFkv2h7MTCG7Bdrf9dRAZqZWeOpqkNERLwGLCD70cB/ALvXMigzM2tsFZOTpIGSvinpQeD6VH90RAzvkOjMzKwhlf0SrqQ/kt13mkH2U+0PdFhUZmbW0Co9vugU4K6Wn2cHkLQL8BlgbETsWevgzMysMVXqEHFnRISk/pK+Lmku8FiaZ2yHRWhmZg2n0pdwJ0m6A5gD9AGOBZ6LiO9FxKMdFJ+ZmTWgSpf1fgH8CfhMRMwDkBQV6puZmbWLSslpe+AoYEp6lNEMoFuHRGVmZg2t0j2nVRFxQUR8mOw3nF4Cnpf0uKQfdliEZmbWcKr9Em5zREyJiCZgNLC2tmGZmVkjq9Qh4mhJn21l0v7A4tqFZGZmja7SmdNXgGtaKb8aOKE24ZiZmVVOTt0iYk1pYUS8jDtGmJlZDVVKTltI2rK0UFIvoHvtQjIzs0ZXKTldDPxe0k4tBZIGAVcAv65tWGZm1sgqfc9pHHAhcKekrQABq4EzI+KXHRGcmZk1pkrJSRFxIXBhupRHRKzumLDMzKyRVUpO/SR9I18g6a3hiDinVkGZmVljq5ScNgN6dVQgZmZmLSolp+ci4nvvpnFJo4CfkSW6iyLizDL1PgX8HvhQy0NmzcyscVXqracK0zZK0mbAecBhwB7AOEl7tFKvF/A14L53szwzM9t0VEpOB7/LtvcFFkfEkxHxGlkX9DGt1Ps+cBZ+Xp+ZmSWVnkr+wrtsewfgmdx4cyp7i6ThwMCIuKFSQ+mHD+dJmrdixYp3GZaZmRVdVU8lrwVJXYBzqOI5fRExNSKaIqKpX79+tQ/OzMzqqpbJaRkwMDc+IJW16AXsBcyRtJTsaeczJTXVMCYzM+sEapmc5gJDJA2W1B0YC8xsmRgRf4+IvhExKCIGAfcCo91bz8zMapacIuINYDJwM/A4MCMiHpN0uqTRtVqumZl1fpW+5/SuRcQsYFZJ2all6o6sZSxmZtZ51K1DhJmZWTlOTmZmVjhOTmZmVjhOTmZmVjhOTmZmVjhOTmZmVjhOTmZmVjhOTmZmVjhOTmZmVjhOTmZmVjhOTmZmVjhOTmZmVjhOTmZmVjhOTmZmVjhOTmZmVjhOTmZmVjhOTmZmVjhOTmZmVjhOTmZmVjhOTmZmVjhOTmZmVjhOTmZmVjhOTmZmVjhOTmZmVjhOTmZmVjhOTmZmVjhOTmZmVjhOTmZmVjhOTmZmVjhOTmZmVjhOTmZmVjhOTmZmVjhOTmZmVjhOTmZmVjg1TU6SRkl6QtJiSae0Mv0bkhZIekTSbZJ2qmU8ZmbWOdQsOUnaDDgPOAzYAxgnaY+Sag8BTRHxT8DvgR/VKh4zM+s8annmtC+wOCKejIjXgCuAMfkKEXFHRLySRu8FBtQwHjMz6yRqmZx2AJ7JjTensnKOBW6sYTxmZtZJdK13AACSjgaagA+XmT4JmASw4447dmBkZmZWD7U8c1oGDMyND0hl65F0CPBfwOiIeLW1hiJiakQ0RURTv379ahKsmZkVRy2T01xgiKTBkroDY4GZ+QqSPgBcSJaY/lbDWMzMrBOpWXKKiDeAycDNwOPAjIh4TNLpkkanaj8GtgKukjRf0swyzZmZWQOp6T2niJgFzCopOzU3fEgtl29mZp2TnxBhZmaF4+RkZmaF4+RkZmaF4+RkZmaF4+RkZmaF4+RkZmaF4+RkZmaF4+RkZmaF4+RkZmaF4+RkZmaF4+RkZmaF4+RkZmaF4+RkZmaF4+RkZmaF4+RkZmaF4+RkZmaF4+RkZmaF4+RkZmaF4+RkZmaF4+RkZmaF4+RkZmaF4+RkZmaF4+RkZmaF4+RkZmaF4+T0Dv3k1kWMOPP2eodhZrZJcnJqg7v+soKfzl7E6rWvr1d+64Ln+fHNC+sUlZnZpqdrvQPobG5+7Hmm/XEpA7ftyT/Wvs6Y8+7hyRVr+MKIwUQEkuodoplZp+fk1AYHDenHrK/25dvX/Jnp9z8NwMrVrzLrqwcxsHfPOkdnZrbp8GW9Nvjj4pV8/Ny7ufHPzzFsh63p1aMrfbfqzuE/v4uf3LqIiKh3iGZmmwQnpzZ4bd2bfHTodtx50kf46NDteE+Pblw7+UB+fOTevPrGm76kZ2bWTnxZrw1G7rYdI3fbboPyUXu9j1F7va8OEZmZbZqcnN6hrx+6K18/dNd6h2FmtknyZT0zMyscJyczMyscJyczMyucmiYnSaMkPSFpsaRTWpm+uaQr0/T7JA2qZTxmZtY51KxDhKTNgPOAQ4FmYK6kmRGxIFftWODFiHi/pLHAWcC/1yqmFoNOuaHWi+gUlp55RL1DMDNrVS3PnPYFFkfEkxHxGnAFMKakzhjg0jT8e+Bg+ctCZmYNr5ZdyXcAnsmNNwP7lasTEW9I+jvQB1iZryRpEjApja6R9ERNIm4wOou+lGxra1zeHyyvHfaHnd7N8jvF95wiYiowtd5xbGokzYuIpnrHYcXg/cHy6r0/1PKy3jJgYG58QCprtY6krsDWwKoaxmRmZp1ALZPTXGCIpMGSugNjgZkldWYCn0/DRwK3h5+eambW8Gp2WS/dQ5oM3AxsBlwcEY9JOh2YFxEzgV8Dv5G0GHiBLIFZx/GlUsvz/mB5dd0f5BMVMzMrGj8hwszMCsfJyczMCsfJaRMi6TRJJ1aYPkFS/9z48ZLa9PvykkZKuv7dxGkdY2Pvr6SLJO3RTsta0x7t2DsnaZakbeodRzWq2V+cnBrLBKB/bvx4oE3JyTqVsu+vpM0i4osljxOzTiwiDo+Il+odR3txcurkJP2XpEWS7gZ2S2X7SLpX0iOSrpG0raQjgSbgd5LmS/oaWaK6Q9Idab6PSfqTpAclXSVpq1Q+StJCSQ8C/1afNbVKJG0p6QZJD0v6s6TvsuH7u0bSFEkPAwdImiOpKTftJ5Iek3SbpH6pfBdJN0l6QNJdkoam8sFpX3lU0hl1Wu2GJelL6f94vqQlku6QtFRS3zT9O+mh23dLmi7pREn9c/PMl7RO0k6SPpEevP2QpNmS3pva6Cfp1rRPXCTpqUrtp/L2218iwq9O+gI+CDxK9un4PcBi4ETgEeDDqc7pwE/T8BygKTf/UqBvGu4L3AlsmcZPBk4FepA9YmoIIGAGcH29192vDfaFTwG/yo1vnX9/U1kAn86Nv7U/pGnj0/CpwC/S8G3AkDS8H9l3ESH7juLn0vBxwJp6b4NGfAHdgLuAT7S838CHgPnpf7cX8BfgxJL5jgNmpOFtebvn9heBKWn4F8C30vCotI9UbL8995dO8fgiK+sg4JqIeAVA0kxgS2CbiPjfVOdS4Koq2tof2AO4Jz17tzvwJ2AosCQi/pKW8Vvefs6hFcejwBRJZ5F9eLirlWcorwP+p8z8bwJXpuHfAlenM+f/B1yVa2vz9HcEWUIE+A3ZLwpYx/sZWQK4TtK5qWwEcG1ErAXWSrouP4OkEcBE4MBUNAC4UtL2ZP/3S1L5gcC/AkTETZJerNR+e+8vTk7WQsCtETFuvUJpnzrFY20QEYskDQcOB86QdFsr1dZGxLpqmyS77P9SRJTbB/wlyTqSNIHs4aqT2zDP9mQPPxgdES2dEs4FzomImZJGAqe9w5DadX/xPafO7U7gk5K2kNSL7NT+ZeBFSQelOp8FWs6iVpOdhtPK+L3ACEnvh7fuYewKLAQGSdol1VsveVkxKOuF+UpE/Bb4MTCcDd/vSrqQPUIM4DPA3RHxD2CJpKPSMiRp71TnHnxMeTMAAAFzSURBVN5+osv4dlgFawNJHyS7hH90RLxZMvke4BOSeqSzmY+nebqRXUU5OSIW5epvzdvPPf18STufTvN+jOzyX9n223t/cXLqxCLiQbJLMQ8DN5I9zxCyHezHkh4B9iG77wQwDbgg3QzdguzxJDdJuiMiVpD15pue5vsTMDSduk8CbkgdIv7WIStnbTUMuF/SfOC7wBnk3t8q5n8Z2FfSn4GP8vY+Mx44NnWieIy3f5Pta8Bxkh4l++kb61iTgd5kHV7mS7qoZUJEzCW7x/MI2XHhUeDvZJfcmoDv5TpF9Cc7U7pK0gOs/xMZ3wM+lvaJo4DlwOoK7UM77i9+fJGZIWlNRGxV7zisfUjaKiLWKPue253ApPRhti1tbA6si+w5qQcAv2y5ZNce7W+M7zmZmW16pir7gnUP4NJ3mDh2BGZI6gK8RtaJoj3br8hnTmZmVji+52RmZoXj5GRmZoXj5GRmZoXj5GRmZoXj5GRmZoXz/wGoI5pxCOmp8gAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}